# ğŸ“š RAG Architecture Documentation

This document provides a detailed overview of how the Retrieval-Augmented Generation (RAG) system is implemented in your project â€” from chunking and embedding to indexing, querying, and summarization.

---

## ğŸ“ 1. RAG Index Creation Pipeline

### ğŸ”„ From Markdown to FAISS

```
ğŸ“ Markdown Files (Tutorials / Docs)
        â”‚
        â–¼
1ï¸âƒ£ split_markdown_to_chunks(text)
    - Splits by headers and paragraphs
    - Returns: [{ id, filename, chunk_index, text }]
        â”‚
        â–¼
2ï¸âƒ£ Save as: rag_tut_chunks.json
        â”‚
        â–¼
3ï¸âƒ£ embedding.py
    - Loads chunks
    - Uses SentenceTransformer('all-MiniLM-L6-v2')
    - Calls embed_text(chunk["text"])
    - Appends .embedding to each chunk
    - Saves: rag_tut_chunks_embedded.json
        â”‚
        â–¼
4ï¸âƒ£ build_faiss_index(chunks)
    - Converts embeddings to np.array
    - Creates FAISS IndexFlatL2
    - Saves: rag_tut_faiss.index
        â”‚
        â””â”€â”€â–¶ save_metadata(chunks, rag_tut_metadata.json)
```

### âœ… Final RAG Artifacts

- `rag_tut_faiss.index` â†’ Used for vector similarity search  
- `rag_tut_metadata.json` â†’ Used for chunk reference & prompt building

---

## ğŸ” 2. RAG Runtime Retrieval & Prompt Generation

```
User asks question
      â”‚
      â–¼
embed_query(question)
  â®• Same SentenceTransformer used
      â”‚
      â–¼
search_index(index, query_embedding)
  â®• Finds similar chunks in FAISS
      â”‚
      â–¼
load_metadata() â†’ retrieves chunk details
      â”‚
      â–¼
build_prompt_docs() or build_prompt_tut()
  â®• Formats:
     - System role
     - Conversation history
     - Code blocks + metadata text
     - New user query
      â”‚
      â–¼
OpenAI Chat Completion (e.g. GPT-4o)
      â”‚
      â–¼
AI Response
```

---

## ğŸ§  3. Auto-Summarization Logic (History Trimming)

To avoid token overflow, chat history is auto-summarized after 4 Q&A pairs.

```
If len(history) >= 4:
    summarize_history(history)
        - Builds summarization prompt
        - Sends to GPT-4o
        - Replaces first 4 pairs with:
            { "question": "Summary so far", "answer": "..." }
        - Returns new trimmed history
```

ğŸ”„ This keeps chat context fresh without losing core info.

---

## ğŸ’¬ 4. Frontend Separation of RAG Assistants

The two assistants (`rag_docs` and `rag_tut`) are isolated in both UI and logic:

```
Docs Assistant           Tutorial Assistant
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DOM: #docs-chat         DOM: #tut-chat
Form: docsForm          Form: tutForm
Input: docsInput        Input: tutInput
API: /chat/rag_docs     API: /chat/rag_tut
```

Each assistant:

- Maintains its own history (via `getChatHistory()`)
- Sends separate POST requests
- Receives isolated responses

â¡ï¸ This ensures clean separation with no shared memory.

---

## âœ… Summary

Your RAG system is:

- âœ… Lightweight (SentenceTransformer-based)  
- âœ… Modular (chunk â†’ embed â†’ index â†’ prompt)  
- âœ… Efficient (summarization keeps context clean)  
- âœ… Scalable (FAISS for fast similarity search)  

Let me know if you want a diagram image or API documentation next!
